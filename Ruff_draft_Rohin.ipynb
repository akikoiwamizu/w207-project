{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18857</th>\n",
       "      <td>An elderly Jewish woman living in Paris was no...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16321</th>\n",
       "      <td>HERE S A REMINDER OF JONATHAN GRUBER INSULTING...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526</th>\n",
       "      <td>The GOP implosion we have been waiting for has...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>While at a rally in Wilmington, North Carolina...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3836</th>\n",
       "      <td>WASHINGTON (Reuters) - Senate Judiciary Commit...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6755</th>\n",
       "      <td>Republican presidential front-runner Donald Tr...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19948</th>\n",
       "      <td>Scum of the earth isn t strong enough for thes...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>Does Donald Trump have tapes of his conversati...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14200</th>\n",
       "      <td>Obama s cowardly backdoor gun confiscation sta...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16253</th>\n",
       "      <td>The employees went on twitter to speak about g...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35918 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text label\n",
       "18857  An elderly Jewish woman living in Paris was no...  fake\n",
       "16321  HERE S A REMINDER OF JONATHAN GRUBER INSULTING...  fake\n",
       "6526   The GOP implosion we have been waiting for has...  fake\n",
       "5118   While at a rally in Wilmington, North Carolina...  fake\n",
       "3836   WASHINGTON (Reuters) - Senate Judiciary Commit...  true\n",
       "...                                                  ...   ...\n",
       "6755   Republican presidential front-runner Donald Tr...  fake\n",
       "19948  Scum of the earth isn t strong enough for thes...  fake\n",
       "1042   Does Donald Trump have tapes of his conversati...  fake\n",
       "14200  Obama s cowardly backdoor gun confiscation sta...  fake\n",
       "16253  The employees went on twitter to speak about g...  fake\n",
       "\n",
       "[35918 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9475</th>\n",
       "      <td>Should legal gun owners be allowed to freely c...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7763</th>\n",
       "      <td>Republican presidential candidate and Texas Se...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9103</th>\n",
       "      <td>It s a rare occasion when a Hollywood legend d...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6661</th>\n",
       "      <td>WASHINGTON (Reuters) - The FBI backs the CIA’s...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18048</th>\n",
       "      <td>Wow! This is one of the most powerful speeches...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13415</th>\n",
       "      <td>GENEVA (Reuters) - The United Nations said on ...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5265</th>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13528</th>\n",
       "      <td>BEIRUT (Reuters) - Lebanon s Prime Minister Sa...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10125</th>\n",
       "      <td>And once again, the snowflakes want to erase p...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>One of President-elect Donald Trump s potentia...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8980 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text label\n",
       "9475   Should legal gun owners be allowed to freely c...  fake\n",
       "7763   Republican presidential candidate and Texas Se...  fake\n",
       "9103   It s a rare occasion when a Hollywood legend d...  fake\n",
       "6661   WASHINGTON (Reuters) - The FBI backs the CIA’s...  true\n",
       "18048  Wow! This is one of the most powerful speeches...  fake\n",
       "...                                                  ...   ...\n",
       "13415  GENEVA (Reuters) - The United Nations said on ...  true\n",
       "5265   WASHINGTON (Reuters) - U.S. President Donald T...  true\n",
       "13528  BEIRUT (Reuters) - Lebanon s Prime Minister Sa...  true\n",
       "10125  And once again, the snowflakes want to erase p...  fake\n",
       "12334  One of President-elect Donald Trump s potentia...  fake\n",
       "\n",
       "[8980 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_data=pd.read_csv('~/Desktop/w207/Final_Project/w207-project/input/fake-and-real-news-dataset/Fake.csv')\n",
    "true_data=pd.read_csv('~/Desktop/w207/Final_Project/w207-project/input/fake-and-real-news-dataset/True.csv')\n",
    "fake_data['label']='fake'\n",
    "true_data['label']='true'\n",
    "fake_corpus=fake_data[['text','label']]\n",
    "true_corpus=true_data[['text','label']]\n",
    "frames=[fake_corpus,true_corpus]\n",
    "data=pd.concat(frames)\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "display(train)\n",
    "display(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'an', 'either', 'this', 'forty', 'rather', 'most', 'themselves', 'everything', 'latter', 'interest', 'nor', 'indeed', 'co', 'almost', 'sometimes', 'have', 'twelve', 'whereafter', 'except', 'would', 'ltd', 'fifteen', 'is', 'up', 'of', 'neither', 'your', 'made', 'some', 'thin', 'call', 'though', 'elsewhere', 'himself', 'again', 'eg', 'been', 'not', 'and', 'other', 'there', 'seemed', 'hereby', 'over', 'cant', 'for', 'he', 'even', 'whereupon', 'find', 'somehow', 'any', 'serious', 'moreover', 'empty', 'him', 'we', 'why', 'ourselves', 'still', 'somewhere', 'sometime', 'one', 'where', 'five', 'ever', 'twenty', 'thereupon', 'describe', 'whenever', 'amoungst', 'yet', 'bill', 'nine', 'seems', 'from', 'whole', 'could', 'perhaps', 'first', 'eight', 'she', 'here', 'full', 'ie', 'are', 'yourselves', 'three', 'get', 'nevertheless', 'all', 'may', 'otherwise', 'thereby', 'afterwards', 'can', 'via', 'latterly', 'detail', 'once', 'those', 'much', 'thereafter', 'de', 'a', 'same', 'which', 'below', 'con', 'must', 'everyone', 'yourself', 'after', 're', 'move', 'whereby', 'out', 'nowhere', 'among', 'became', 'beyond', 'keep', 'washington', 'back', 'herein', 'see', 'at', 'in', 'few', 'another', 'never', 'give', 'while', 'please', 'hers', 'also', 'formerly', 'hasnt', 'six', 'along', 'each', 'above', 'anything', 'anyhow', 'am', 'sincere', 'within', 'if', 'they', 'reuters', 'seem', 'itself', 'mostly', 'what', 'show', 'noone', 'go', 'several', 'four', 'least', 'through', 'etc', 'found', 'yours', 'throughout', 'hereupon', 'namely', 'were', 'well', 'had', 'name', 'nobody', 'front', 'hereafter', 'whence', 'that', 'couldnt', 'more', 'others', 'thence', 'during', 'often', 'but', 'none', 'whither', 'besides', 'it', 'about', 'enough', 'whom', 'further', 'hundred', 'always', 'or', 'has', 'no', 'its', 'before', 'two', 'towards', 'whose', 'due', 'i', 'only', 'therefore', 'amount', 'both', 'his', 'many', 'under', 'top', 'between', 'fifty', 'fill', 'hence', 'down', 'side', 'part', 'mill', 'ours', 'until', 'per', 'our', 'cry', 'eleven', 'former', 'toward', 'with', 'too', 'behind', 'anyone', 'off', 'then', 'less', 'whether', 'whatever', 'now', 'should', 'wherever', 'seeming', 'me', 'take', 'across', 'these', 'against', 'might', 'put', 'their', 'thru', 'own', 'than', 'do', 'so', 'such', 'around', 'becoming', 'however', 'wherein', 'every', 'since', 'beforehand', 'un', 'you', 'whoever', 'alone', 'bottom', 'myself', 'was', 'ten', 'her', 'how', 'my', 'them', 'done', 'system', 'into', 'very', 'whereas', 'us', 'be', 'thus', 'sixty', 'becomes', 'being', 'meanwhile', 'else', 'something', 'who', 'when', 'amongst', 'beside', 'will', 'together', 'on', 'thick', 'because', 'by', 'become', 'although', 'anywhere', 'last', 'upon', 'without', 'inc', 'onto', 'nothing', 'third', 'someone', 'therein', 'fire', 'mine', 'anyway', 'as', 'next', 'already', 'cannot', 'the', 'to', 'everywhere', 'herself'})\n",
      "fake    18756\n",
      "true    17162\n",
      "Name: label, dtype: int64 \n",
      " Shape of Data is  (35918, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "additional_words=['reuters','washington']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(additional_words)\n",
    "print(stop_words)\n",
    "print(train['label'].value_counts(),'\\n Shape of Data is ',train.shape)\n",
    "corpus=train['text']\n",
    "labels=train['label']\n",
    "test_corpus=test['text']\n",
    "test_labels=test['label']\n",
    "cv=CountVectorizer(ngram_range=(3,3),lowercase=True,token_pattern='\\w{3,}',stop_words=stop_words)\n",
    "train_cv=cv.fit_transform(corpus)\n",
    "test_cv=cv.transform(test_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic F1_score 0.91701\n"
     ]
    }
   ],
   "source": [
    "log_model=LogisticRegression(solver=\"liblinear\", multi_class=\"auto\")\n",
    "log_model.fit(train_cv,labels)\n",
    "pred_labels=log_model.predict(test_cv)\n",
    "f1_score=round(metrics.f1_score(test_labels,pred_labels,average=\"weighted\"),5)\n",
    "weights=log_model.coef_\n",
    "print('Logistic F1_score',f1_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show K1 and F1-Score for KNN Model \n",
      " Model  K-value  F1-Score\n",
      " KNN     1.0     0.4301 \n",
      " KNN     3.0     0.3778 \n",
      " KNN     5.0     0.3727 \n",
      " KNN     7.0     0.3715 \n",
      " KNN     9.0     0.3663 \n",
      " KNN    11.0     0.3650 \n",
      " KNN    13.0     0.3646 \n"
     ]
    }
   ],
   "source": [
    "knn_values=[1,3,5,7,9,11,13]\n",
    "df_k_stats=pd.DataFrame()\n",
    "knn_stats={'Model':'KNN','K-value':None,'F1-Score':None} #Create dictionary to hold KNN stats to append with dataframe\n",
    "for i in knn_values:\n",
    "    knn_stats['K-value'] = i #Store K-value within dictionary\n",
    "    knn_model = KNeighborsClassifier(n_neighbors = i) #Create KNN model\n",
    "    knn_model.fit(train_cv,labels) #Fit model on training data from CountVectorizer fit and transform\n",
    "    knn_pred=knn_model.predict(test_cv) #Predict on dev dataset\n",
    "    knn_model_f1=metrics.f1_score(test_labels,knn_pred,average=\"weighted\") #Get f1-score based on the dev labels and predicted values\n",
    "    knn_stats['F1-Score'] = round(knn_model_f1,4) #Make it pretty\n",
    "    df_k_stats = df_k_stats.append(knn_stats, ignore_index=True) #place the values in dataframe\n",
    "print('Show K1 and F1-Score for KNN Model','\\n',df_k_stats.to_string(index=False,justify='center'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Show Alpha and F1-Score for Naive-Bayes Model \n",
      "    Model      Alpha   F1-Score\n",
      "Naive-Bayes  0.00001   0.9688 \n",
      "Naive-Bayes  0.00010   0.9691 \n",
      "Naive-Bayes  0.00100   0.9694 \n",
      "Naive-Bayes  0.01000   0.9715 \n",
      "Naive-Bayes  0.10000   0.9725 \n",
      "Naive-Bayes  1.00000   0.9736 \n",
      "Naive-Bayes  2.00000   0.9758 \n",
      "Naive-Bayes  5.00000   0.9737 \n",
      "Naive-Bayes 10.00000   0.9723 \n"
     ]
    }
   ],
   "source": [
    "df_alpha_stats=pd.DataFrame() #Create dataframe to hold alpha stats\n",
    "alpha=[.00001,.0001,.001,.01,.1,1,2,5,10] #alpha-values for testing for Naive-Bayes model\n",
    "for i in alpha:\n",
    "    nb_stats={'Model':'Naive-Bayes','Alpha':None,'F1-Score':None} #Create dictionary to hold Naive-Bayes stats to append with dataframe\n",
    "    nb_stats['Alpha']=i #Store alpha-value within dictionary\n",
    "    nb_model=MultinomialNB(alpha=i) #Create Naive-Bayes model based on alpha value\n",
    "    nb_model.fit(train_cv,labels) #Fit model on training data from CountVectorizer fit and transform\n",
    "    nb_pred=nb_model.predict(test_cv) #Predict on dev dataset\n",
    "    nb_model_f1=metrics.f1_score(test_labels,nb_pred,average=\"weighted\") #Get f1-score based on the dev labels and predicted values\n",
    "    nb_stats['F1-Score']=round(nb_model_f1,4) #Make it pretty\n",
    "    df_alpha_stats=df_alpha_stats.append(nb_stats,ignore_index=True)  #place the values in dataframe\n",
    "print('\\nShow Alpha and F1-Score for Naive-Bayes Model','\\n',df_alpha_stats.to_string(index=False,justify='center'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>president emmanuel macron</td>\n",
       "      <td>1.667915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white house said</td>\n",
       "      <td>1.703371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chancellor angela merkel</td>\n",
       "      <td>1.815570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>state news agency</td>\n",
       "      <td>1.836994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>president barack obama</td>\n",
       "      <td>1.838863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>president tayyip erdogan</td>\n",
       "      <td>1.928007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>told news conference</td>\n",
       "      <td>2.190746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>president robert mugabe</td>\n",
       "      <td>2.206320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>prime minister theresa</td>\n",
       "      <td>2.749074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>president donald trump</td>\n",
       "      <td>3.711844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       words      true\n",
       "0  president emmanuel macron  1.667915\n",
       "1           white house said  1.703371\n",
       "2   chancellor angela merkel  1.815570\n",
       "3          state news agency  1.836994\n",
       "4     president barack obama  1.838863\n",
       "5   president tayyip erdogan  1.928007\n",
       "6       told news conference  2.190746\n",
       "7    president robert mugabe  2.206320\n",
       "8     prime minister theresa  2.749074\n",
       "9     president donald trump  3.711844"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00848072 0.10487506 0.15931626 ... 0.92878343 0.01243837 0.50925231]\n"
     ]
    }
   ],
   "source": [
    "weights=log_model.coef_ #Get the weights of the logistic regression\n",
    "sorted_weights=np.argsort(weights)[:,-10:].flatten() #sort them in order to get the top 5 weights per topic\n",
    "topic_words=cv.get_feature_names_out()[sorted_weights] #Grab the words from the top 5 weights \n",
    "categories=['true','fake']\n",
    "topics=categories#set the topics to place in the dataframe\n",
    "df_topics=pd.DataFrame() #create dataframe\n",
    "df_topics['words']=pd.DataFrame(topic_words) #create the column of words for the top 20 words in each category\n",
    "for i in range(1): #create for loop to go through the topics\n",
    "    specific_weight=weights[i][sorted_weights] #Get the specific weight for that word\n",
    "    df_topics[topics[i]]=pd.DataFrame(specific_weight,columns=[topics[i]]) #Place the word and topics in the row\n",
    "display(df_topics) #showcase the topics and the words for the feature\n",
    "y_pred_prob = log_model.predict_proba(test_cv)[:, 1]\n",
    "print(y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohin/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>russian president vladimir</td>\n",
       "      <td>-11.663681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>speaker paul ryan</td>\n",
       "      <td>-11.657781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>secretary state rex</td>\n",
       "      <td>-11.593243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>state rex tillerson</td>\n",
       "      <td>-11.591406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>president vladimir putin</td>\n",
       "      <td>-11.422942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>president elect donald</td>\n",
       "      <td>-11.364234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>elect donald trump</td>\n",
       "      <td>-11.361314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white house said</td>\n",
       "      <td>-11.270786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>president barack obama</td>\n",
       "      <td>-10.123659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>president donald trump</td>\n",
       "      <td>-9.446495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        words       true\n",
       "0  russian president vladimir -11.663681\n",
       "1           speaker paul ryan -11.657781\n",
       "2         secretary state rex -11.593243\n",
       "3         state rex tillerson -11.591406\n",
       "4    president vladimir putin -11.422942\n",
       "5      president elect donald -11.364234\n",
       "6          elect donald trump -11.361314\n",
       "7            white house said -11.270786\n",
       "8      president barack obama -10.123659\n",
       "9      president donald trump  -9.446495"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights=nb_model.coef_ #Get the weights of the logistic regression\n",
    "sorted_weights=np.argsort(weights)[:,-10:].flatten() #sort them in order to get the top 5 weights per topic\n",
    "topic_words=cv.get_feature_names_out()[sorted_weights] #Grab the words from the top 5 weights \n",
    "categories=['true','fake']\n",
    "topics=categories#set the topics to place in the dataframe\n",
    "df_topics=pd.DataFrame() #create dataframe\n",
    "df_topics['words']=pd.DataFrame(topic_words) #create the column of words for the top 20 words in each category\n",
    "for i in range(1): #create for loop to go through the topics\n",
    "    specific_weight=weights[i][sorted_weights] #Get the specific weight for that word\n",
    "    df_topics[topics[i]]=pd.DataFrame(specific_weight,columns=[topics[i]]) #Place the word and topics in the row\n",
    "display(df_topics) #showcase the topics and the words for the feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
